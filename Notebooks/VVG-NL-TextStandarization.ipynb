{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# VINCENT VAN GOGH STANDARD DATA TEXT PROCESS\n",
    "\n",
    "This script takes the gallery's text from the local data folder and process it to an standard representation with Natural Language Processing and Word2Vec methods.\n",
    "\n",
    "The process goes as follows:\n",
    "\n",
    "1. Load the CSV into a pandas DataFrame.\n",
    "2. Transform text columns into words lists.\n",
    "\n",
    "    1.1 Clear words in text list.\n",
    "    \n",
    "4. Remove unnecesary words from words list with the stop word dictionary.\n",
    "5. \n",
    "6. Transform URLs columns into standard categories.\n",
    "\n",
    "https://statsmaths.github.io/stat289-f18/solutions/tutorial19-gensim.html\n",
    "\n",
    "\n",
    "each TXT page as a record in the input representation of the model.\n",
    "2.\tRemoving unnecessary words with the Spanish stop dictionary.\n",
    "3.\tRecognizing a set of unique words in documents.\n",
    "4.\tTransforming the unique words into columns of the input model.\n",
    "5.\tVectorizing each word of the document by frequency of appearance (word2vec).\n",
    "\n",
    "**NOTE:** Because GitHub has limited storage capabilities and the digital archive data is private, the data in the folder _\\\\Data\\\\_ is just a sample for the code to work without errors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading nlkt data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ===============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook varlable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with the OCR transcrived txt data\n",
    "prepf = \"Prep\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "stdf = \"Std\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "\n",
    "# dataframe file name\n",
    "small_fn = \"VVG-Gallery-Text-Data-Small\" + \".\" + fext\n",
    "large_fn = \"VVG-Gallery-Text-Data-Large\" + \".\" + fext\n",
    "\n",
    "\n",
    "# regex for _TEXT\n",
    "text_re = u\"\\w+_TEXT\"\n",
    "\n",
    "# regex for ID\n",
    "id_re = u\"ID{1}\"\n",
    "\n",
    "# regex for others (URLs|Categories)\n",
    "cat_re = u\"\\b(?!(ID{1}|\\w+_TEXT))\\b(\\w+\\W+)+\"\n",
    "cat_re = u\"ID{1}(^\\w+( \\w+)*$)\"\n",
    "\n",
    "# default values\n",
    "work_fn = small_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords files\n",
    "basicStopWords = \"mlt-uniandes-spanish-stop-words\" + \".txt\"\n",
    "compositeStopWords = \"composite-nltk-spanish-stop-words\" + \".txt\"\n",
    "\n",
    "# default dataframe schema\n",
    "dfSchema =[\n",
    "    \"ID\",               # unique key for the text file\n",
    "    \"FILE_PATH\",        # text file local path\n",
    "    \"DOC_NAME\",         # name of the text original document\n",
    "    \"TEXT\",             # OCR extractec text\n",
    "    \"AUTHOR\",           # author of the document\n",
    "    \"LABEL\",            # learning target label, associated with the AUTHOR\n",
    "    \"CLEAN_TEXT\",       # cleaned text extracted from the document\n",
    "    \"SENTENCES\",        # text divided by sentences\n",
    "    \"NUM_SENTENCES\",    # number of sentences in the text\n",
    "    \"WORDS\",            # text divided by words\n",
    "    \"NUM_WORDS\",        # number of words in the text\n",
    "    \"TOKENS\",           # unique tokens extracted from the text\n",
    "    \"NUM_TOKENS\",       # number of unique of tokens in the text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the CSV file into pandas\n",
    "# read an existing CSV fileto update the dataframe\n",
    "fn_path = os.path.join(os.getcwd(), dataf, prepf, work_fn)\n",
    "print(fn_path)\n",
    "text_df = pd.read_csv(\n",
    "                fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the df columns\n",
    "df_cols = list(text_df)\n",
    "\n",
    "# getting the text columns\n",
    "text_r = re.compile(text_re)\n",
    "text_cols = list(filter(text_r.match, df_cols))\n",
    "\n",
    "# getting the ID column\n",
    "id_r = re.compile(id_re)\n",
    "id_cols = list(filter(id_r.match, df_cols))\n",
    "\n",
    "# getting the URLs/Category columns\n",
    "cat_r = re.compile(cat_re)\n",
    "cat_cols = list(filter(cat_r.match, df_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the original working text\n",
    "text_corpus = list(text_df[text_cols[0]])\n",
    "print(len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to working text\n",
    "text_clean = list()\n",
    "for text in text_corpus:\n",
    "    text = text.lower()\n",
    "    text_clean.append(text)\n",
    "\n",
    "print(len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleaning and preprocessing text for word2vec\n",
    "i = 0\n",
    "for i in range(0, len(text_clean)):\n",
    "    text = text_clean[i]\n",
    "    # removing special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    # finding missing points between numbers\n",
    "    text = re.sub(r\"(\\d{1,3}) (\\d{1,2})\", r\"\\1.\\2\", text)\n",
    "    # removing excessive spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text_clean[i] = text\n",
    "    i = i + 1\n",
    "\n",
    "print(len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenising text\n",
    "text_tokens = list()\n",
    "\n",
    "for text in text_clean:\n",
    "    text = text.split()\n",
    "    text_tokens.append(text)\n",
    "    # print(text)\n",
    "\n",
    "print(len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "text_nsw_tokens = list()\n",
    "\n",
    "for tokens in text_tokens:\n",
    "\n",
    "    clear_tokens = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token in stopwords.words('english'):\n",
    "            clear_tokens.append(token)\n",
    "    \n",
    "    ttokens = copy.deepcopy(clear_tokens)\n",
    "    text_nsw_tokens.append(ttokens)\n",
    "    # print(clear_tokens)\n",
    "\n",
    "print(len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lematization of the text\n",
    "text_lemmas = list()\n",
    "token_lematizer = WordNetLemmatizer()\n",
    "\n",
    "for tokens in text_nsw_tokens:\n",
    "\n",
    "    lemma_tokens = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        \n",
    "        ans = token_lematizer.lemmatize(token)\n",
    "        lemma_tokens.append(ans)\n",
    "\n",
    "    tlemmas = copy.deepcopy(lemma_tokens)\n",
    "    text_lemmas.append(tlemmas)\n",
    "\n",
    "print(len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"TOKENS\"] = text_tokens\n",
    "text_df[\"PREP_TOKENS\"] = text_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vvg_dict = gensim.corpora.Dictionary(text_lemmas)\n",
    "print(vvg_dict)\n",
    "vvg_dict.save(os.path.join(\"Data\",\"VVG-gallery-text.dict\"))\n",
    "# pprint.pprint(vvg_dict.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text representation to numeric representation\n",
    "text_bows = list()\n",
    "text_idxs = list()\n",
    "\n",
    "for lemmas in text_lemmas:\n",
    "\n",
    "    # bow loose the order/semantic\n",
    "    t_bow = vvg_dict.doc2bow(lemmas, allow_update=True)\n",
    "    text_bows.append(t_bow)\n",
    "    # idz keeps the order/semantic\n",
    "    t_idx = vvg_dict.doc2idx(lemmas)\n",
    "    text_idxs.append(t_idx)\n",
    "\n",
    "print(len(text_bows), len(text_idxs), len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "tfidf = gensim.models.TfidfModel(text_idxs, dictionary=vvg_dict, normalize=True)\n",
    "corpus_tfidf = tfidf[text_bows]\n",
    "print(len(corpus_tfidf), len(text_bows), len(text_idxs), len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"BOWS_TOKENS\"] = text_bows\n",
    "text_df[\"IDX_TOKENS\"] = text_idxs\n",
    "text_df[\"TFIDF_TOKENS\"] = corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking everything is okey\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the dense vector standar representantion of the text\n",
    "text_dvector = list()\n",
    "\n",
    "# iterating in each text with the tfidf word bag\n",
    "for t_idtokens, tfidf_tokens in zip(text_idxs, corpus_tfidf):\n",
    "    # print(\"===============================\")\n",
    "    # print(len(tidxs), len(ttfidf))\n",
    "    # print(type(tidxs), type(ttfidf))\n",
    "    # dense vector representation\n",
    "    tdvect = list()\n",
    "\n",
    "    # creating the dense representation for each text\n",
    "    for t_token in t_idtokens:\n",
    "\n",
    "        # transforming the tfidf into dict\n",
    "        tokens_dict = dict(tfidf_tokens)\n",
    "        \n",
    "        # looking for each word\n",
    "        if t_token in tokens_dict.keys():\n",
    "            temp = tokens_dict.get(t_token)\n",
    "            # appending std word representation into array\n",
    "            tdvect.append(temp)\n",
    "\n",
    "    # copying std dense vector into corpus column\n",
    "    ans = copy.deepcopy(tdvect)\n",
    "    text_dvector.append(ans)\n",
    "\n",
    "# checking the size of all columna\n",
    "print(len(text_dvector), len(corpus_tfidf), len(text_bows), len(text_idxs), len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))\n",
    "\n",
    "# adding the dense representation into the dataframe\n",
    "text_df[\"STD_DVEC_TOKENS\"] = text_dvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking everything is okey\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the CSV file into pandas\n",
    "# writing an existing CSV fileto update the dataframe\n",
    "target_fn = \"std-\" + work_fn\n",
    "fn_tpath = os.path.join(os.getcwd(), dataf, stdf, target_fn)\n",
    "print(fn_tpath)\n",
    "text_df.to_csv(fn_tpath,\n",
    "                sep=\",\",\n",
    "                index=False,\n",
    "                encoding=\"utf-8\",\n",
    "                mode=\"w\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont remember for what i did this\n",
    "# sim_index = gensim.similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(vvg_dict))"
   ]
  }
 ]
}