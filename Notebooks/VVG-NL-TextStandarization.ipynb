{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# VINCENT VAN GOGH STANDARD DATA TEXT PROCESS\n",
    "\n",
    "This script takes the gallery's text from the local data folder and process it to an standard representation with Natural Language Processing and Word2Vec methods.\n",
    "\n",
    "The process goes as follows:\n",
    "\n",
    "1. Load the CSV into a pandas DataFrame.\n",
    "2. Transform text columns into words lists.\n",
    "\n",
    "    1.1 Clear words in text list.\n",
    "    \n",
    "4. Remove unnecesary words from words list with the stop word dictionary.\n",
    "5. \n",
    "6. Transform URLs columns into standard categories.\n",
    "\n",
    "https://statsmaths.github.io/stat289-f18/solutions/tutorial19-gensim.html\n",
    "\n",
    "\n",
    "each TXT page as a record in the input representation of the model.\n",
    "2.\tRemoving unnecessary words with the Spanish stop dictionary.\n",
    "3.\tRecognizing a set of unique words in documents.\n",
    "4.\tTransforming the unique words into columns of the input model.\n",
    "5.\tVectorizing each word of the document by frequency of appearance (word2vec).\n",
    "\n",
    "**NOTE:** Because GitHub has limited storage capabilities and the digital archive data is private, the data in the folder _\\\\Data\\\\_ is just a sample for the code to work without errors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"\n",
    "* Copyright 2020, Maestria de Humanidades Digitales,\n",
    "* Universidad de Los Andes\n",
    "*\n",
    "* Developed for the Msc graduation project in Digital Humanities\n",
    "*\n",
    "* This program is free software: you can redistribute it and/or modify\n",
    "* it under the terms of the GNU General Public License as published by\n",
    "* the Free Software Foundation, either version 3 of the License, or\n",
    "* (at your option) any later version.\n",
    "*\n",
    "* This program is distributed in the hope that it will be useful,\n",
    "* but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "* GNU General Public License for more details.\n",
    "*\n",
    "* You should have received a copy of the GNU General Public License\n",
    "* along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "# ===============================\n",
    "# native python libraries\n",
    "# ===============================\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# ===============================\n",
    "# extension python libraries\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# downloading nlkt data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# developed python libraries\n",
    "# ===============================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook varlable definitions\n",
    "# root folder\n",
    "dataf = \"Data\"\n",
    "\n",
    "# subfolder with the OCR transcrived txt data\n",
    "prepf = \"Prep\"\n",
    "\n",
    "#  subfolder with the CSV files containing the ML pandas dataframe\n",
    "stdf = \"Std\"\n",
    "\n",
    "# dataframe file extension\n",
    "fext = \"csv\"\n",
    "\n",
    "# dataframe file name\n",
    "small_fn = \"VVG-Gallery-Text-Data-Small\" + \".\" + fext\n",
    "large_fn = \"VVG-Gallery-Text-Data-Large\" + \".\" + fext\n",
    "\n",
    "\n",
    "# regex for _TEXT\n",
    "text_re = u\"\\w+_TEXT\"\n",
    "\n",
    "# regex for ID\n",
    "id_re = u\"ID{1}\"\n",
    "\n",
    "# regex for others (URLs|Categories)\n",
    "cat_re = u\"\\b(?!(ID{1}|\\w+_TEXT))\\b(\\w+\\W+)+\"\n",
    "cat_re = u\"ID{1}(^\\w+( \\w+)*$)\"\n",
    "\n",
    "# default values\n",
    "work_fn = small_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords files\n",
    "basicStopWords = \"mlt-uniandes-spanish-stop-words\" + \".txt\"\n",
    "compositeStopWords = \"composite-nltk-spanish-stop-words\" + \".txt\"\n",
    "\n",
    "# positive and negative data subfolders, each category is repeated in all 4 previous subfolders\n",
    "positiveFolder = \"01-Mechas\"\n",
    "negativeFolder = \"00-Others\"\n",
    "\n",
    "# default dataframe schema\n",
    "dfSchema =[\n",
    "    \"ID\",               # unique key for the text file\n",
    "    \"FILE_PATH\",        # text file local path\n",
    "    \"DOC_NAME\",         # name of the text original document\n",
    "    \"TEXT\",             # OCR extractec text\n",
    "    \"AUTHOR\",           # author of the document\n",
    "    \"LABEL\",            # learning target label, associated with the AUTHOR\n",
    "    \"CLEAN_TEXT\",       # cleaned text extracted from the document\n",
    "    \"SENTENCES\",        # text divided by sentences\n",
    "    \"NUM_SENTENCES\",    # number of sentences in the text\n",
    "    \"WORDS\",            # text divided by words\n",
    "    \"NUM_WORDS\",        # number of words in the text\n",
    "    \"TOKENS\",           # unique tokens extracted from the text\n",
    "    \"NUM_TOKENS\",       # number of unique of tokens in the text\n",
    "]\n",
    "\n",
    "initColumns = [\n",
    "    dfSchema[0],\n",
    "    dfSchema[1],\n",
    "    dfSchema[2],    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\Felipe\\Documents\\GitHub\\sa-artea\\VVG-Gallery-StdDataProcessor\\Notebooks\\Data\\Prep\\VVG-Gallery-Text-Data-Small.csv\n"
     ]
    }
   ],
   "source": [
    "# loading the CSV file into pandas\n",
    "\n",
    "# read an existing CSV fileto update the dataframe\n",
    "fn_path = os.path.join(os.getcwd(), dataf, prepf, work_fn)\n",
    "print(fn_path)\n",
    "text_df = pd.read_csv(\n",
    "                fn_path,\n",
    "                sep=\",\",\n",
    "                encoding=\"utf-8\",\n",
    "                engine=\"python\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the df columns\n",
    "df_cols = list(text_df)\n",
    "\n",
    "# getting the text columns\n",
    "text_r = re.compile(text_re)\n",
    "text_cols = list(filter(text_r.match, df_cols))\n",
    "\n",
    "# getting the ID column\n",
    "id_r = re.compile(id_re)\n",
    "id_cols = list(filter(id_r.match, df_cols))\n",
    "\n",
    "# getting the URLs/Category columns\n",
    "cat_r = re.compile(cat_re)\n",
    "cat_cols = list(filter(cat_r.match, df_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# getting the original working text\n",
    "text_corpus = list(text_df[text_cols[0]])\n",
    "print(len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59\n"
     ]
    }
   ],
   "source": [
    "# to working text\n",
    "text_clean = list()\n",
    "for text in text_corpus:\n",
    "    text = text.lower()\n",
    "    text_clean.append(text)\n",
    "\n",
    "print(len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59\n"
     ]
    }
   ],
   "source": [
    "# cleaning and preprocessing text for word2vec\n",
    "i = 0\n",
    "for i in range(0, len(text_clean)):\n",
    "    text = text_clean[i]\n",
    "    # removing special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "    # finding missing points between numbers\n",
    "    text = re.sub(r\"(\\d{1,3}) (\\d{1,2})\", r\"\\1.\\2\", text)\n",
    "    # removing excessive spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text_clean[i] = text\n",
    "    i = i + 1\n",
    "\n",
    "print(len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59 59\n"
     ]
    }
   ],
   "source": [
    "# tokenising text\n",
    "text_tokens = list()\n",
    "\n",
    "for text in text_clean:\n",
    "    text = text.split()\n",
    "    text_tokens.append(text)\n",
    "    # print(text)\n",
    "\n",
    "print(len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59 59 59\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "text_nsw_tokens = list()\n",
    "\n",
    "for tokens in text_tokens:\n",
    "\n",
    "    clear_tokens = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token in stopwords.words('english'):\n",
    "            clear_tokens.append(token)\n",
    "    \n",
    "    ttokens = copy.deepcopy(clear_tokens)\n",
    "    text_nsw_tokens.append(ttokens)\n",
    "    # print(clear_tokens)\n",
    "\n",
    "print(len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59 59 59 59\n"
     ]
    }
   ],
   "source": [
    "# lematization of the text\n",
    "text_lemmas = list()\n",
    "token_lematizer = WordNetLemmatizer()\n",
    "\n",
    "for tokens in text_nsw_tokens:\n",
    "\n",
    "    lemma_tokens = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        \n",
    "        ans = token_lematizer.lemmatize(token)\n",
    "        lemma_tokens.append(ans)\n",
    "\n",
    "    tlemmas = copy.deepcopy(lemma_tokens)\n",
    "    text_lemmas.append(tlemmas)\n",
    "\n",
    "print(len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"TOKENS\"] = text_tokens\n",
    "text_df[\"PREP_TOKENS\"] = text_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            ID                                          CORE_TEXT  \\\n",
       "0  s0004V1962r  Head of a Woman Vincent van Gogh (1853 - 1890)...   \n",
       "1   s0006V1962  Head of a Woman Vincent van Gogh (1853 - 1890)...   \n",
       "2   s0010V1962  Portrait of an Old Woman Vincent van Gogh (185...   \n",
       "3   s0056V1962  Torso of Venus Vincent van Gogh (1853 - 1890),...   \n",
       "4   s0058V1962  Woman with a Mourning Shawl Vincent van Gogh (...   \n",
       "\n",
       "                                            EXT_TEXT  \\\n",
       "0        F0388r JH0782 s0004V1962r 43.5 cm x 36.2 cm   \n",
       "1  F0160 JH0722 s0006V1962 43.2 cm x 30.0 cm, 2.2...   \n",
       "2  F0174 JH0978 s0010V1962 50.5 cm x 39.8 cm, 68....   \n",
       "3  F0216a JH1054 s0056V1962 46.0 cm x 38.0 cm, 55...   \n",
       "4  F0161 JH0788 s0058V1962 45.5 cm x 33.0 cm, 60 ...   \n",
       "\n",
       "                               complementary colours  \\\n",
       "0                                          localhost   \n",
       "1  https://www.vangoghmuseum.nl/en/stories/lookin...   \n",
       "2                                          localhost   \n",
       "3                                          localhost   \n",
       "4                                          localhost   \n",
       "\n",
       "                                 this torso of Venus  \\\n",
       "0                                          localhost   \n",
       "1                                          localhost   \n",
       "2                                          localhost   \n",
       "3  https://www.vangoghmuseum.nl/en/collection/s01...   \n",
       "4                                          localhost   \n",
       "\n",
       "                                                drew Van Gogh wrote  \\\n",
       "0                                          localhost      localhost   \n",
       "1                                          localhost      localhost   \n",
       "2                                          localhost      localhost   \n",
       "3                                          localhost      localhost   \n",
       "4  https://www.vangoghmuseum.nl/en/collection/d00...      localhost   \n",
       "\n",
       "  standing torso of Venus   he wrote The Potato Eaters  ...       1884  \\\n",
       "0               localhost  localhost         localhost  ...  localhost   \n",
       "1               localhost  localhost         localhost  ...  localhost   \n",
       "2               localhost  localhost         localhost  ...  localhost   \n",
       "3               localhost  localhost         localhost  ...  localhost   \n",
       "4               localhost  localhost         localhost  ...  localhost   \n",
       "\n",
       "        1887 animal art    drawing       1890  cityscape       1881  \\\n",
       "0  localhost  localhost  localhost  localhost  localhost  localhost   \n",
       "1  localhost  localhost  localhost  localhost  localhost  localhost   \n",
       "2  localhost  localhost  localhost  localhost  localhost  localhost   \n",
       "3  localhost  localhost  localhost  localhost  localhost  localhost   \n",
       "4  localhost  localhost  localhost  localhost  localhost  localhost   \n",
       "\n",
       "    Brussels                                             TOKENS  \\\n",
       "0  localhost  [head, of, a, woman, vincent, van, gogh, 1853,...   \n",
       "1  localhost  [head, of, a, woman, vincent, van, gogh, 1853,...   \n",
       "2  localhost  [portrait, of, an, old, woman, vincent, van, g...   \n",
       "3  localhost  [torso, of, venus, vincent, van, gogh, 1853, 1...   \n",
       "4  localhost  [woman, with, a, mourning, shawl, vincent, van...   \n",
       "\n",
       "                                         PREP_TOKENS  \n",
       "0  [head, woman, vincent, van, gogh, 1853, 1890, ...  \n",
       "1  [head, woman, vincent, van, gogh, 1853, 1890, ...  \n",
       "2  [portrait, old, woman, vincent, van, gogh, 185...  \n",
       "3  [torso, venus, vincent, van, gogh, 1853, 1890,...  \n",
       "4  [woman, mourning, shawl, vincent, van, gogh, 1...  \n",
       "\n",
       "[5 rows x 52 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>CORE_TEXT</th>\n      <th>EXT_TEXT</th>\n      <th>complementary colours</th>\n      <th>this torso of Venus</th>\n      <th>drew</th>\n      <th>Van Gogh wrote</th>\n      <th>standing torso of Venus</th>\n      <th>he wrote</th>\n      <th>The Potato Eaters</th>\n      <th>...</th>\n      <th>1884</th>\n      <th>1887</th>\n      <th>animal art</th>\n      <th>drawing</th>\n      <th>1890</th>\n      <th>cityscape</th>\n      <th>1881</th>\n      <th>Brussels</th>\n      <th>TOKENS</th>\n      <th>PREP_TOKENS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>s0004V1962r</td>\n      <td>Head of a Woman Vincent van Gogh (1853 - 1890)...</td>\n      <td>F0388r JH0782 s0004V1962r 43.5 cm x 36.2 cm</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>[head, of, a, woman, vincent, van, gogh, 1853,...</td>\n      <td>[head, woman, vincent, van, gogh, 1853, 1890, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>s0006V1962</td>\n      <td>Head of a Woman Vincent van Gogh (1853 - 1890)...</td>\n      <td>F0160 JH0722 s0006V1962 43.2 cm x 30.0 cm, 2.2...</td>\n      <td>https://www.vangoghmuseum.nl/en/stories/lookin...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>[head, of, a, woman, vincent, van, gogh, 1853,...</td>\n      <td>[head, woman, vincent, van, gogh, 1853, 1890, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>s0010V1962</td>\n      <td>Portrait of an Old Woman Vincent van Gogh (185...</td>\n      <td>F0174 JH0978 s0010V1962 50.5 cm x 39.8 cm, 68....</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>[portrait, of, an, old, woman, vincent, van, g...</td>\n      <td>[portrait, old, woman, vincent, van, gogh, 185...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>s0056V1962</td>\n      <td>Torso of Venus Vincent van Gogh (1853 - 1890),...</td>\n      <td>F0216a JH1054 s0056V1962 46.0 cm x 38.0 cm, 55...</td>\n      <td>localhost</td>\n      <td>https://www.vangoghmuseum.nl/en/collection/s01...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>[torso, of, venus, vincent, van, gogh, 1853, 1...</td>\n      <td>[torso, venus, vincent, van, gogh, 1853, 1890,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>s0058V1962</td>\n      <td>Woman with a Mourning Shawl Vincent van Gogh (...</td>\n      <td>F0161 JH0788 s0058V1962 45.5 cm x 33.0 cm, 60 ...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>https://www.vangoghmuseum.nl/en/collection/d00...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>...</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>localhost</td>\n      <td>[woman, with, a, mourning, shawl, vincent, van...</td>\n      <td>[woman, mourning, shawl, vincent, van, gogh, 1...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 52 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary(660 unique tokens: ['1', '11', '16', '1853', '1885']...)\n{'1': 0,\n '10': 75,\n '11': 1,\n '12': 622,\n '13.0': 599,\n '14': 520,\n '16': 2,\n '17': 613,\n '1853': 3,\n '1880': 623,\n '1881': 624,\n '1884': 273,\n '1884.1885': 426,\n '1885': 4,\n '1886': 160,\n '1887': 327,\n '1888': 521,\n '1889': 522,\n '1890': 5,\n '1891': 6,\n '19.8': 580,\n '1902': 523,\n '1902.03': 524,\n '1903.04': 525,\n '1914': 625,\n '1920': 626,\n '1925': 7,\n '1930': 8,\n '1931': 572,\n '1952': 9,\n '1956': 526,\n '1960': 10,\n '1962': 11,\n '1964': 527,\n '1965': 528,\n '1970': 195,\n '1972': 614,\n '1973': 12,\n '1981': 627,\n '1989': 628,\n '1990': 629,\n '1994': 13,\n '1st': 14,\n '2': 15,\n '2.2': 76,\n '20.8': 585,\n '2004': 630,\n '2005': 631,\n '21': 16,\n '22': 573,\n '22.1': 615,\n '24.0': 328,\n '24.4': 218,\n '24.6': 597,\n '24.9': 576,\n '25': 17,\n '254': 632,\n '26.6': 616,\n '26.8': 398,\n '26.9': 611,\n '27.0': 421,\n '27.1': 396,\n '27.2': 496,\n '27.3': 609,\n '28': 18,\n '29.1': 600,\n '29.5': 300,\n '2nd': 19,\n '30.0': 77,\n '30.5': 568,\n '30.8': 574,\n '31': 269,\n '31.3': 424,\n '31.4': 575,\n '31.5': 605,\n '31.7': 598,\n '31.8': 603,\n '32.1': 274,\n '32.2': 577,\n '32.7': 494,\n '32.9': 329,\n '33.0': 196,\n '33.1': 508,\n '33.3': 302,\n '33.5': 427,\n '33.7': 242,\n '34.4': 586,\n '34.5': 368,\n '34.8': 270,\n '35.0': 422,\n '35.2': 219,\n '35.3': 397,\n '36.2': 20,\n '37.7': 301,\n '38.0': 161,\n '38.1': 347,\n '38.3': 581,\n '38.5': 447,\n '38.8': 425,\n '39.7': 618,\n '39.8': 125,\n '4': 633,\n '40': 78,\n '40.8': 476,\n '40.9': 509,\n '41.0': 495,\n '42.0': 303,\n '42.2': 271,\n '42.5': 275,\n '42.6': 634,\n '42.7': 428,\n '43.2': 79,\n '43.5': 21,\n '43.7': 612,\n '43.8': 332,\n '44.3': 610,\n '44.4': 243,\n '44.6': 588,\n '45.5': 197,\n '46.0': 162,\n '46.3': 448,\n '46.4': 348,\n '47': 333,\n '47.3': 606,\n '47.5': 561,\n '47.6': 620,\n '47.7': 569,\n '47.8': 529,\n '48.9': 604,\n '50': 276,\n '50.5': 126,\n '50.9': 619,\n '54.6': 589,\n '61.1': 608,\n '62.0': 562,\n '74.750': 635,\n 'academy': 399,\n 'achieve': 400,\n 'activity': 334,\n 'administered': 22,\n 'advice': 277,\n 'agreement': 23,\n 'aid': 401,\n 'aimed': 278,\n 'almost': 477,\n 'also': 429,\n 'among': 497,\n 'amsterdam': 24,\n 'anatomy': 402,\n 'angle': 163,\n 'annet': 636,\n 'antiquity': 498,\n 'antwerp': 127,\n 'apartment': 164,\n 'appear': 80,\n 'appearance': 369,\n 'appointment': 244,\n 'april': 81,\n 'arm': 587,\n 'around': 304,\n 'art': 128,\n 'artist': 165,\n 'august': 530,\n 'author': 245,\n 'auvers': 590,\n 'away': 166,\n 'background': 129,\n 'bakel': 531,\n 'bargue': 591,\n 'based': 478,\n 'beard': 246,\n 'beautiful': 430,\n 'become': 305,\n 'began': 279,\n 'bequeathed': 532,\n 'besides': 247,\n 'better': 82,\n 'bit': 306,\n 'black': 307,\n 'blob': 449,\n 'block': 601,\n 'blondeau': 637,\n 'blue': 349,\n 'body': 167,\n 'bonger': 25,\n 'bonnet': 130,\n 'bought': 638,\n 'brabant': 431,\n 'breda': 533,\n 'bremmer': 534,\n 'brother': 26,\n 'brush': 168,\n 'brushstrokes': 131,\n 'brussels': 639,\n 'bun': 450,\n 'bussum': 27,\n 'c': 535,\n 'called': 370,\n 'canvas': 28,\n 'cap': 132,\n 'carbentus': 536,\n 'cardboard': 169,\n 'care': 371,\n 'carefully': 198,\n 'case': 170,\n 'cast': 330,\n 'century': 83,\n 'chalk': 563,\n 'charcoal': 570,\n 'cheaper': 510,\n 'choosing': 220,\n 'chose': 372,\n 'christie': 640,\n 'city': 221,\n 'classical': 171,\n 'close': 373,\n 'clothing': 133,\n 'cm': 29,\n 'coarse': 374,\n 'cobalt': 350,\n 'collection': 30,\n 'colour': 84,\n 'come': 248,\n 'communion': 375,\n 'compelling': 249,\n 'complementary': 85,\n 'concentrated': 86,\n 'concluded': 31,\n 'consignment': 537,\n 'constantly': 308,\n 'contemporary': 222,\n 'contrast': 134,\n 'contrasting': 479,\n 'copy': 480,\n 'could': 451,\n 'countless': 309,\n 'country': 376,\n 'countrywoman': 335,\n 'couvreur': 538,\n 'crayon': 578,\n 'created': 403,\n 'creating': 351,\n 'curve': 172,\n 'cut': 452,\n 'dark': 135,\n 'daughter': 641,\n 'day': 310,\n 'de': 87,\n 'dealer': 136,\n 'death': 32,\n 'december': 33,\n 'decided': 173,\n 'deep': 352,\n 'depicted': 404,\n 'depiction': 311,\n 'depth': 353,\n 'different': 174,\n 'difficult': 432,\n 'direction': 405,\n 'distance': 481,\n 'donated': 34,\n 'done': 312,\n 'dot': 313,\n 'drawing': 314,\n 'dress': 453,\n 'drew': 199,\n 'earring': 454,\n 'earth': 88,\n 'easy': 455,\n 'eater': 89,\n 'ecorche': 406,\n 'effect': 407,\n 'eleven': 499,\n 'end': 90,\n 'ended': 315,\n 'entrusted': 35,\n 'essential': 336,\n 'eternally': 433,\n 'everything': 175,\n 'exercise': 337,\n 'expect': 377,\n 'expensive': 250,\n 'experimenting': 500,\n 'eye': 316,\n 'eyelash': 482,\n 'eyelid': 317,\n 'f': 642,\n 'face': 137,\n 'facial': 378,\n 'falie': 200,\n 'family': 91,\n 'farm': 318,\n 'farmed': 379,\n 'farmworkers': 92,\n 'favourite': 354,\n 'feature': 223,\n 'february': 251,\n 'fell': 456,\n 'female': 138,\n 'fewer': 355,\n 'figure': 93,\n 'finding': 252,\n 'fine': 139,\n 'first': 224,\n 'five': 176,\n 'flat': 380,\n 'focused': 140,\n 'followed': 177,\n 'forehead': 381,\n 'form': 483,\n 'found': 280,\n 'foundation': 36,\n 'fragment': 617,\n 'friend': 281,\n 'full': 253,\n 'funeral': 201,\n 'gallery': 643,\n 'gelink': 644,\n 'geneve': 645,\n 'girl': 564,\n 'given': 37,\n 'giving': 225,\n 'glance': 226,\n 'glittering': 457,\n 'gogh': 38,\n 'good': 338,\n 'gordina': 94,\n 'green': 95,\n 'grey': 141,\n 'groot': 96,\n 'h': 539,\n 'hague': 540,\n 'hair': 142,\n 'half': 592,\n 'han': 646,\n 'hand': 202,\n 'hard': 254,\n 'hat': 582,\n 'head': 39,\n 'headdress': 434,\n 'heard': 143,\n 'heir': 647,\n 'help': 511,\n 'holbein': 648,\n 'hollmann': 649,\n 'home': 512,\n 'hoped': 97,\n 'horse': 513,\n 'hugo': 255,\n 'human': 484,\n 'identity': 282,\n 'idol': 571,\n 'illusion': 356,\n 'immediately': 178,\n 'impressionist': 650,\n 'impressive': 256,\n 'included': 227,\n 'including': 98,\n 'individual': 319,\n 'inheritance': 541,\n 'inherited': 40,\n 'instead': 514,\n 'intended': 382,\n 'interest': 383,\n 'intrigued': 435,\n 'jacob': 651,\n 'jan': 542,\n 'january': 41,\n 'janus': 543,\n 'jewellery': 458,\n 'jo': 42,\n 'jr': 544,\n 'july': 43,\n 'june': 44,\n 'kees': 545,\n 'kind': 99,\n 'kneeling': 408,\n 'know': 283,\n 'known': 436,\n 'kroller': 652,\n 'kunsthandel': 546,\n 'kunstzalen': 547,\n 'lady': 459,\n 'land': 100,\n 'laren': 45,\n 'large': 437,\n 'last': 284,\n 'late': 257,\n 'later': 460,\n 'least': 285,\n 'lecorche': 565,\n 'left': 179,\n 'leg': 584,\n 'leiden': 548,\n 'letter': 286,\n 'life': 228,\n 'light': 203,\n 'like': 144,\n 'liked': 145,\n 'lip': 384,\n 'lithographic': 579,\n 'little': 287,\n 'live': 288,\n 'lived': 229,\n 'loan': 46,\n 'local': 146,\n 'look': 204,\n 'looking': 385,\n 'loose': 230,\n 'lot': 180,\n 'low': 386,\n 'made': 101,\n 'mager': 549,\n 'make': 102,\n 'makeup': 461,\n 'male': 423,\n 'man': 258,\n 'management': 47,\n 'many': 339,\n 'march': 48,\n 'margin': 231,\n 'master': 147,\n 'may': 49,\n 'melt': 485,\n 'men': 148,\n 'met': 462,\n 'meyer': 653,\n 'meyers': 654,\n 'mid': 181,\n 'might': 387,\n 'milo': 621,\n 'miscellaneous': 602,\n 'model': 149,\n 'modelled': 103,\n 'modern': 232,\n 'moral': 233,\n 'mother': 550,\n 'mourning': 205,\n 'mouwen': 551,\n 'move': 552,\n 'moving': 463,\n 'mr': 655,\n 'much': 515,\n 'muller': 656,\n 'muscle': 409,\n 'museum': 50,\n 'must': 357,\n 'name': 289,\n 'nature': 388,\n 'needed': 464,\n 'netherlands': 51,\n 'network': 486,\n 'never': 465,\n 'nineteenth': 234,\n 'nl': 320,\n 'nlg': 657,\n 'nothing': 358,\n 'november': 259,\n 'nude': 593,\n 'nuenen': 52,\n 'number': 501,\n 'ochre': 206,\n 'october': 290,\n 'odd': 389,\n 'oil': 53,\n 'oise': 594,\n 'old': 150,\n 'oldenzeel': 553,\n 'one': 104,\n 'opening': 54,\n 'orange': 487,\n 'owned': 502,\n 'p': 554,\n 'paint': 182,\n 'painted': 183,\n 'painter': 105,\n 'painting': 106,\n 'panel': 555,\n 'paper': 566,\n 'parent': 291,\n 'paris': 55,\n 'part': 359,\n 'partly': 438,\n 'passionately': 340,\n 'peasant': 107,\n 'peeping': 207,\n 'pencil': 595,\n 'people': 108,\n 'perfect': 360,\n 'perfectly': 466,\n 'permanent': 56,\n 'persuaded': 467,\n 'physiognomy': 390,\n 'pinkish': 468,\n 'placed': 260,\n 'placing': 57,\n 'plaster': 184,\n 'play': 410,\n 'portrait': 151,\n 'portray': 109,\n 'portraying': 292,\n 'pose': 261,\n 'posed': 110,\n 'potato': 111,\n 'practice': 361,\n 'practiced': 516,\n 'practise': 185,\n 'practising': 208,\n 'preparatory': 341,\n 'prepare': 112,\n 'presented': 235,\n 'preservation': 58,\n 'preserved': 517,\n 'probably': 113,\n 'produce': 342,\n 'produced': 186,\n 'project': 293,\n 'prostitute': 236,\n 'pupil': 321,\n 'purple': 488,\n 'putting': 362,\n 'rather': 503,\n 'ratified': 209,\n 'realized': 59,\n 'really': 439,\n 'reason': 391,\n 'red': 114,\n 'regarded': 343,\n 'remained': 440,\n 'remind': 115,\n 'renaissance': 504,\n 'represented': 322,\n 'respectable': 469,\n 'returned': 294,\n 'right': 323,\n 'rijksmuseum': 60,\n 'rotterdam': 556,\n 'rough': 152,\n 'safekeeping': 557,\n 'sale': 658,\n 'schrauwen': 558,\n 'sculpture': 187,\n 'seated': 567,\n 'see': 441,\n 'seem': 392,\n 'september': 61,\n 'series': 188,\n 'service': 659,\n 'shadow': 153,\n 'shawl': 210,\n 'shifted': 324,\n 'shortly': 470,\n 'show': 262,\n 'shown': 411,\n 'sign': 237,\n 'since': 62,\n 'single': 471,\n 'sitting': 472,\n 'sketch': 607,\n 'skin': 412,\n 'small': 489,\n 'society': 238,\n 'sold': 154,\n 'sometime': 331,\n 'sometimes': 116,\n 'son': 63,\n 'space': 363,\n 'special': 117,\n 'specifically': 393,\n 'splendid': 263,\n 'spot': 211,\n 'spring': 212,\n 'standing': 364,\n 'state': 64,\n 'stedelijk': 65,\n 'stick': 155,\n 'story': 473,\n 'strong': 213,\n 'study': 118,\n 'studying': 413,\n 'subject': 239,\n 'succeeded': 240,\n 'suggest': 490,\n 'suggested': 156,\n 'suggesting': 414,\n 'suitable': 264,\n 'summer': 214,\n 'sur': 596,\n 'survived': 344,\n 'table': 442,\n 'taken': 119,\n 'taught': 518,\n 'technique': 505,\n 'tell': 295,\n 'temporarily': 296,\n 'tendon': 415,\n 'theo': 66,\n 'theos': 67,\n 'thick': 394,\n 'thickness': 416,\n 'thin': 491,\n 'thing': 365,\n 'thomas': 559,\n 'thorough': 189,\n 'thought': 265,\n 'time': 157,\n 'told': 474,\n 'tomorrow': 266,\n 'top': 583,\n 'torso': 190,\n 'total': 345,\n 'traditional': 417,\n 'transferred': 68,\n 'tried': 418,\n 'triplex': 272,\n 'turned': 191,\n 'two': 366,\n 'type': 297,\n 'u': 298,\n 'unknown': 299,\n 'unusual': 395,\n 'use': 506,\n 'used': 192,\n 'using': 158,\n 'van': 69,\n 'variety': 443,\n 'venus': 193,\n 'victor': 267,\n 'villager': 346,\n 'vincent': 70,\n 'virtue': 475,\n 'visible': 419,\n 'volume': 241,\n 'wanted': 120,\n 'warm': 492,\n 'way': 215,\n 'wear': 444,\n 'wearing': 216,\n 'well': 268,\n 'white': 159,\n 'whole': 194,\n 'whose': 445,\n 'wickerwork': 507,\n 'widow': 71,\n 'willem': 72,\n 'winter': 446,\n 'wisselingh': 560,\n 'without': 420,\n 'woman': 73,\n 'word': 367,\n 'work': 121,\n 'worked': 325,\n 'worker': 326,\n 'working': 122,\n 'worn': 217,\n 'would': 123,\n 'wrote': 124,\n 'x': 74,\n 'year': 519,\n 'zoom': 493}\n"
     ]
    }
   ],
   "source": [
    "vvg_dict = gensim.corpora.Dictionary(text_lemmas)\n",
    "print(vvg_dict)\n",
    "vvg_dict.save(os.path.join(\"Data\",\"VVG-gallery-text.dict\"))\n",
    "pprint.pprint(vvg_dict.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59 59 59 59 59 59 59\n"
     ]
    }
   ],
   "source": [
    "# text representation to numeric representation\n",
    "text_bows = list()\n",
    "text_idxs = list()\n",
    "\n",
    "for lemmas in text_lemmas:\n",
    "\n",
    "    # bow loose the order/semantic\n",
    "    t_bow = vvg_dict.doc2bow(lemmas, allow_update=True)\n",
    "    text_bows.append(t_bow)\n",
    "    # idz keeps the order/semantic\n",
    "    t_idx = vvg_dict.doc2idx(lemmas)\n",
    "    text_idxs.append(t_idx)\n",
    "\n",
    "print(len(text_bows), len(text_idxs), len(text_lemmas), len(text_nsw_tokens), len(text_tokens), len(text_clean), len(text_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 0.024662054234269034),\n",
       " (1, 3.5607149544744794),\n",
       " (2, 3.8826430493618416),\n",
       " (4, 3.425436095839058),\n",
       " (6, 0.04975303519709973),\n",
       " (7, 0.04975303519709973),\n",
       " (8, 3.8826430493618416),\n",
       " (9, 3.5607149544744794),\n",
       " (10, 3.0752881273042374),\n",
       " (11, 0.04932410846853807),\n",
       " (12, 0.04932410846853807),\n",
       " (13, 0.024662054234269034),\n",
       " (14, 7.121429908948959),\n",
       " (15, 0.0739861627028071),\n",
       " (16, 0.09950607039419947),\n",
       " (17, 0.04975303519709973),\n",
       " (18, 2.712718047919529),\n",
       " (19, 3.5607149544744794),\n",
       " (20, 5.882643049361842),\n",
       " (21, 5.882643049361842),\n",
       " (22, 0.04975303519709973),\n",
       " (23, 0.04975303519709973),\n",
       " (25, 0.09950607039419947),\n",
       " (26, 0.04975303519709973),\n",
       " (27, 0.04975303519709973),\n",
       " (28, 1.634715535918256),\n",
       " (30, 0.024662054234269034),\n",
       " (31, 0.04975303519709973),\n",
       " (33, 3.7652860987236823),\n",
       " (34, 6.59536109728137),\n",
       " (35, 0.04975303519709973),\n",
       " (36, 0.1479723254056142),\n",
       " (37, 0.04932410846853807),\n",
       " (39, 2.0752881273042374),\n",
       " (42, 0.09950607039419947),\n",
       " (43, 0.0739861627028071),\n",
       " (44, 0.04932410846853807),\n",
       " (45, 0.19901214078839893),\n",
       " (46, 0.0739861627028071),\n",
       " (47, 0.04975303519709973),\n",
       " (48, 0.6347155359182558),\n",
       " (49, 1.9515049075066453),\n",
       " (51, 0.04975303519709973),\n",
       " (52, 2.0752881273042374),\n",
       " (53, 1.1822033312207492),\n",
       " (54, 0.024662054234269034),\n",
       " (55, 0.09950607039419947),\n",
       " (56, 0.024662054234269034),\n",
       " (57, 0.04975303519709973),\n",
       " (58, 0.04975303519709973),\n",
       " (59, 0.04975303519709973),\n",
       " (60, 0.04932410846853807),\n",
       " (61, 0.04975303519709973),\n",
       " (62, 0.04932410846853807),\n",
       " (63, 0.024662054234269034),\n",
       " (64, 0.09950607039419947),\n",
       " (65, 0.04932410846853807),\n",
       " (66, 0.1492591055912992),\n",
       " (67, 0.04975303519709973),\n",
       " (68, 0.04975303519709973),\n",
       " (71, 0.024662054234269034),\n",
       " (72, 0.0739861627028071),\n",
       " (73, 1.7951802081115018)]"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# train the model\n",
    "tfidf = gensim.models.TfidfModel(text_idxs, dictionary=vvg_dict, normalize=True)\n",
    "corpus_tfidf = tfidf[text_bows]\n",
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_index = gensim.similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(vvg_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['obj', 'corpus', 'chunksize', 'metadata'])"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "a = tfidf._apply(text_idxs)\n",
    "a.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['id2word', 'wlocal', 'wglobal', 'normalize', 'num_docs', 'num_nnz', 'idfs', 'smartirs', 'slope', 'pivot', 'eps', 'cfs', 'dfs', 'term_lens'])"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "tfidf.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function gensim.models.tfidfmodel.df2idf(docfreq, totaldocs, log_base=2.0, add=0.0)>"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# tfidf.idfs\n",
    "tfidf.wglobal\n",
    "\n",
    "# print(list(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}